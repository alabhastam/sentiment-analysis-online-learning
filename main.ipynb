{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-20T19:40:34.871152Z","iopub.execute_input":"2025-09-20T19:40:34.871461Z","iopub.status.idle":"2025-09-20T19:40:36.708063Z","shell.execute_reply.started":"2025-09-20T19:40:34.871435Z","shell.execute_reply":"2025-09-20T19:40:36.706830Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"<div style=\"background-color:#1e1e1e; color:#d4d4d4; padding:25px; border-radius:10px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n\n<h1 style=\"color:#61dafb; text-align:center;\">üì° Real-Time Sentiment Analysis with Online Learning</h1>\n\n<h2 style=\"color:#56b6c2;\">1Ô∏è‚É£ Motivation</h2>\n<p>\nIn the age of social media, data arrives continuously ‚Äî tweets, comments, and posts are generated every second. \nTraditional <b>batch learning</b> models require retraining from scratch when new data arrives, which:\n</p>\n<ul>\n<li>Consumes significant resources (CPU, memory, time).</li>\n<li>Cannot adapt fast enough to sudden changes in trends or topics.</li>\n<li>Is impractical for endless data streams.</li>\n</ul>\n<p>\n<b>Online Learning</b> (or Incremental Learning) solves this by updating the model immediately as new data arrives, without reprocessing all past data. \nThis makes it ideal for <span style=\"color:#98c379;\">real-time sentiment analysis</span>, fraud detection, and dynamic recommendation systems.\n</p>\n\n<h2 style=\"color:#56b6c2;\">2Ô∏è‚É£ Project Goal</h2>\n<p>\nWe will build an <b>online sentiment analysis system</b> that processes tweets in small batches, \npredicts their sentiment (<span style=\"color:#98c379;\">Positive</span> / <span style=\"color:#e06c75;\">Negative</span>), \nand continuously improves as new data arrives.\n</p>\n\n<h2 style=\"color:#56b6c2;\">3Ô∏è‚É£ Dataset ‚Äî Sentiment140</h2>\n<ul>\n<li><b>Source:</b> <a href=\"https://www.kaggle.com/datasets/kazanova/sentiment140\" style=\"color:#61dafb;\">Sentiment140 Dataset</a></li>\n<li><b>Size:</b> 1.6 million labeled tweets</li>\n<li><b>Labels:</b> 0 = Negative, 4 = Positive (will convert 4 ‚Üí 1 for binary classification)</li>\n<li><b>Content:</b> Raw tweet text, no emojis, collected via Twitter API</li>\n</ul>\n\n<h2 style=\"color:#56b6c2;\">4Ô∏è‚É£ Approach</h2>\n<ol>\n<li><b>Load and preprocess</b> the Sentiment140 dataset.</li>\n<li>Use <code>HashingVectorizer</code> for text-to-vector transformation (efficient in streaming scenarios).</li>\n<li>Train an <b>SGDClassifier</b> model incrementally using <code>partial_fit</code>.</li>\n<li>Simulate a stream of tweets in <b>mini-batches</b> (batch size: 10,000 rows).</li>\n<li>Record and plot <b>accuracy over time</b> to see model improvement.</li>\n</ol>\n\n<h2 style=\"color:#56b6c2;\">5Ô∏è‚É£ Why Online Learning for This Task?</h2>\n<ul>\n<li>Handles <b>continuous inflow</b> of social media data.</li>\n<li>Works within <b>limited memory</b> ‚Äî no need to store all past tweets.</li>\n<li><b>Fast adaptation</b> to changing topics, slang, and trends.</li>\n</ul>\n\n<h2 style=\"color:#56b6c2;\">6Ô∏è‚É£ Expected Outcome</h2>\n<p>\nBy the end of this notebook, we‚Äôll have a working real-time sentiment classifier that can learn from new tweets without full retraining. \nThis approach can be extended to live Twitter APIs and other streaming data sources.\n</p>\n\n<hr style=\"border: 1px solid #333; margin: 20px 0;\">\n<p style=\"text-align:center; color:#888;\">\n<strong>Author's Note:</strong> This notebook uses a simulated streaming setup on the Sentiment140 dataset. The same code can be adapted to handle live Twitter data.\n</p>\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#1e1e1e; color:#d4d4d4; padding:20px; border-radius:8px; font-family:Segoe UI;\">\n<h2 style=\"color:#56b6c2;\">Step 1 ‚Äî Import Libraries & Load Dataset</h2>\n<p>We start by importing the core Python libraries for online learning and sentiment analysis:</p>\n<ul>\n<li><b>pandas</b> ‚Äî to handle tabular data</li>\n<li><b>numpy</b> ‚Äî for numeric operations</li>\n<li><b>sklearn</b> ‚Äî for vectorization and the online learning model</li>\n<li><b>matplotlib</b> ‚Äî for accuracy visualization</li>\n</ul>\n<p>We then load a labeled Twitter dataset, where each sample contains the text of the tweet and a label:\n<span style=\"color:#98c379;\">1 for positive</span> and <span style=\"color:#e06c75;\">0 for negative</span>.</p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T19:51:37.225048Z","iopub.execute_input":"2025-09-20T19:51:37.225552Z","iopub.status.idle":"2025-09-20T19:51:37.231943Z","shell.execute_reply.started":"2025-09-20T19:51:37.225525Z","shell.execute_reply":"2025-09-20T19:51:37.230893Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"<div style=\"background-color:#1e1e1e; color:#d4d4d4; padding:25px; border-radius:10px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n\n<h2 style=\"color:#56b6c2;\">Step 2 ‚Äî Load & Preprocess the Dataset</h2>\n\n<p>\nIn this step, we load the <b>Sentiment140 dataset</b> and prepare it for streaming-based online learning. \nSince the dataset includes extra columns we don‚Äôt need, we will:\n</p>\n\n<ol>\n<li>Read the CSV with <code>pandas</code> using the correct encoding (<code>latin-1</code>) to handle special characters.</li>\n<li>Keep only the <b>label</b> and <b>tweet text</b> columns.</li>\n<li>Convert labels:\n    <span style=\"color:#e06c75;\">0</span> ‚Üí Negative,\n    <span style=\"color:#98c379;\">4</span> ‚Üí Positive ‚Üí mapped to <span style=\"color:#98c379;\">1</span> for binary classification.</li>\n<li>Shuffle the dataset to simulate random arrival of tweets in a data stream.</li>\n<li>Split into training and test datasets, where the test set remains fixed for evaluation across streaming batches.</li>\n</ol>\n\n<p>\nThis preprocessing ensures our model sees tweets in a mixed, unpredictable order ‚Äî closer to a real online environment.\n</p>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Load Sentiment140 dataset\n# Kaggle path: /kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n# Columns: target, ids, date, flag, user, text\n\ndf = pd.read_csv(\n    \"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\",\n    encoding=\"latin-1\",\n    header=None\n)\n\n# Keep only sentiment and text\ndf = df[[0, 5]]\ndf.columns = ['label', 'text']\n\n# Map labels: 0 -> 0 (negative), 4 -> 1 (positive)\ndf['label'] = df['label'].replace({4: 1})\n\n# Shuffle dataset to simulate random streaming\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Train-test split (e.g., 90% train for streaming simulation, 10% fixed test set)\ntrain_size = int(len(df) * 0.9)\ntrain_df = df.iloc[:train_size]\ntest_df = df.iloc[train_size:]\n\n# Show dataset info\nprint(train_df.head())\nprint(train_df['label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T19:52:49.948111Z","iopub.execute_input":"2025-09-20T19:52:49.948475Z","iopub.status.idle":"2025-09-20T19:52:58.065506Z","shell.execute_reply.started":"2025-09-20T19:52:49.948450Z","shell.execute_reply":"2025-09-20T19:52:58.064560Z"}},"outputs":[{"name":"stdout","text":"   label                                               text\n0      0             @chrishasboobs AHHH I HOPE YOUR OK!!! \n1      0  @misstoriblack cool , i have no tweet apps  fo...\n2      0  @TiannaChaos i know  just family drama. its la...\n3      0  School email won't open  and I have geography ...\n4      0                             upper airways problem \nlabel\n1    720035\n0    719965\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4}]}